{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mamba in Needle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mamba Architecture](https://raw.githubusercontent.com/rishi1001/mamba_needle/refs/heads/main/images/mamba.png?token=GHSAT0AAAAAAC2WHEE7MJET7W6IAWQO2LIKZ2YXOMQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bF2iD92AYoiD",
    "outputId": "1b541e25-978c-4d6c-f5ee-8d015e14581c"
   },
   "outputs": [],
   "source": [
    "# clone our project repo\n",
    "%cd /content\n",
    "!git clone https://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@github.com/rishi1001/mamba_needle.git\n",
    "%cd /content/mamba_needle\n",
    "\n",
    "# install some requirements\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avb03Jp_aAgo",
    "outputId": "a0a44205-2ccb-49e2-b8ae-f5eba07025d3"
   },
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8ahP5B2aJWh",
    "outputId": "b1cf7b7f-42fb-4b67-fab6-b27b1b47c8ed"
   },
   "outputs": [],
   "source": [
    "# set approrpiate environment variables\n",
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd\n",
    "\n",
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQXRi3otYc7I"
   },
   "source": [
    "## **Mamba: A State-Space Model Architecture**\n",
    "\n",
    "In the homeworks for this course, we implemented three sequence models in the Needle library: recurrent neural networks (RNNs), long short-term memory (LSTM) networks and transformers. In this project, we add a fourth model to this list: Mamba.\n",
    "\n",
    "Mamba is a state-space model architecture designed to combine the high accuracy of transformers with the efficiency of linear RNNs. This report explains the key principles behind Mamba, its efficiency, scalability, and how it achieves constant time inference.\n",
    "\n",
    "### **Efficiency**\n",
    "\n",
    "* **Linear Time Training**: Unlike transformers, which scale quadratically with sequence length, Mamba optimizes training to be linear.  \n",
    "* **Constant Time Inference**: Enables real-time applications by reducing computational overhead.\n",
    "\n",
    "### **Scalability**\n",
    "\n",
    "* **Long-Range Tasks**: Handles long sequences efficiently.  \n",
    "* **Fixed Computation per Timestep**: Ensures scalability while avoiding quadratic complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwBIHkk8Yc7J"
   },
   "source": [
    "The state-space model (SSM) forms the backbone of Mamba's architecture:\n",
    "\n",
    "1. **State Equation**: \\[ h'(t) \\= A h(t) \\+ B x(t) \\]  \n",
    "2. **Output Equation**: \\[ y(t) \\= C h(t) \\+ D x(t) \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7rc7viIYc7J"
   },
   "source": [
    "\n",
    "![SSM Equations](https://raw.githubusercontent.com/rishi1001/mamba_needle/refs/heads/main/images/ssm_equations.jpg?token=GHSAT0AAAAAAC2WHEE6QPFRDU7VYS7DUN4GZ2YXVPA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZtpdPNeYc7J"
   },
   "source": [
    "The state equation describes how the state changes as a function of time and as a function of its input x.\n",
    "\n",
    "* **h’(t)**: the state update  \n",
    "* **Ah(t)**: how the current state changes over time  \n",
    "* **Bx(t)**: how the input influences the state\n",
    "\n",
    "The output equation describes how the output changes as a function of the state and as a function of its input x.\n",
    "\n",
    "* **y(t)**: the output  \n",
    "* **Ch(t)**: how the current state affects the output  \n",
    "* **Dx(t)**: how the input directly influences the output\n",
    "\n",
    "We transform the continuous h(t) from above to a discrete form. A continuous h(t) is difficult to work with and also does not represent the input since inputs are usually discrete (i.e. sequence of text). This process can be compared to transforming the area under a curve to a Riemann sum. The discretized matrices A and B are calculated using the formula shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAtSQzP5Yc7J"
   },
   "source": [
    "So, now, we have the discretized version :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM_BjSO-Yc7K"
   },
   "source": [
    "Note that SSMs can be calculated in the form of kernels. We can precaulcate the kernels and apply them parallely over the input, similar to CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlGwby00Yc7K"
   },
   "source": [
    "### **Mamba Block**\n",
    "\n",
    "The Mamba model is composed of a sequence of Mamba blocks. The architecture of the Mamba block is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThXqRNU_Yc7K"
   },
   "source": [
    "\n",
    "![Mamba Block](https://raw.githubusercontent.com/rishi1001/mamba_needle/refs/heads/main/images/mamba_block.jpg?token=GHSAT0AAAAAAC2WHEE6HL6TYTNLBUGHHMHGZ2YXXQQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqxh2RQGYc7K"
   },
   "source": [
    "We implemented the following blocks:\n",
    "\n",
    "* **RMSNorm**: Applied over the last (not batch size or sequence length) dimension. Divides each input by the L2 norm of the last dimension. A small value (epsilon) can be added to the denominator to ensure there will be no divide by 0 errors.  \n",
    "* **Conv1D**: While Conv2D was applied over the last 2 dimensions, Conv1D is applied over the last dimension. It takes input of shape (batch\\_size, in\\_channels, sequence\\_length) and slides a kernel of size (kernel\\_size, in\\_channels, out\\_channels) over the length of the sequence. We added a groups argument, which determines how many groups to divide the inputs and outputs into. Each group is convolved separately, so the inputs only affect outputs in the same group.  \n",
    "* **SiLU**: Input times logistic sigmoid.   \n",
    "* **Selective SSM** (see next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Gy7iELYc7K"
   },
   "source": [
    "### **Selective SSM**\n",
    "\n",
    "SSMs are poor at selectively remembering specific inputs. For instance, a SSM would fail at copying specific parts of an input and outputting them in order. This is because the matrices A, B, and C are the same for each token the SSM generates, resulting in the inability to treat tokens differently.\n",
    "\n",
    "A selective SSM solves this problem by making dt, B, and C dependent on the input. Recall that Bx(t) represents how the input influences the state and Ch(t) represents how the current state affects the output. The architecture for the Selective SSM is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUoZquYBYc7K"
   },
   "source": [
    "\n",
    "![Selective SSM Block](https://github.com/rishi1001/mamba_needle/blob/main/images/selective_ssm.jpg?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2pfMqNTYc7K"
   },
   "source": [
    "### **Sequential Selective Scan**\n",
    "\n",
    "The selective scan implements the updates to the state equation and the output equation (see State Space Models section). The state equation is dependent on the previous state, which allows us to formulate the equation updates as a RNN as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0bmZOvyYc7K"
   },
   "source": [
    "Note that, since these matrices are now dynamic, they cannot be calculated using the convolution representation since it assumes a fixed kernel(like in SSMs). We can only use the recurrent representation and lose the parallelization the convolution provides. So, we use a prefix parallel scan algorithm during training as described ahead, and recurrent representation while inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tSKkHqyYc7K"
   },
   "source": [
    "# **Parallel prefix scans**\n",
    "\n",
    "So far, we have seen an implementation of a Mamba that performs the forward pass sequentially; in linear time with respect to the length of the input sequence. According to the equations provided above, this is the natural implementation. And even this implementation provides time complexity improvements over the quadratic time transformer model.\n",
    "\n",
    "However, the authors of the mamba paper argued that the computations can also equivalently be written as prefix sums. And given previous research on parallel prefix sum algorithms, they showed that they could further reduce the time complexity.\n",
    "\n",
    "The parallel prefix sum algorithm we used was initially described by CMU Professor Guy Blelloch and further documented on NVIDIA’s website\\[1\\]. It’s divided up into two phases: the “up sweep” phase and the “down sweep” phase.\n",
    "\n",
    "Each phase has on the order of log\\_2(L) steps, where L is the length of the input sequence. And the actual number of computations done across all steps — referred to on the website as “work efficiency” — is also linear, meaning that asymptotically no extra computations are done in this algorithm versus the sequential implementation.\n",
    "\n",
    "With all this being said, we needed to augment this base integer prefix sum algorithm in two ways. First: we needed to add support for multiple dimensions given that our input isn’t just a 1D array.\n",
    "\n",
    "The solution for the first problem is simple: we just launch a separate thread block for every dimension that we compute like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q6n3kX4Yc7K"
   },
   "source": [
    "The second problem is that we’re not just computing a basic prefix sum; each step includes a multiplication and an addition. Let’s take a second look at the to make this more concrete:\n",
    "\n",
    "ℎ𝑡 \\= 𝑨\\_tℎ𝑡−1 \\+ 𝑩\\_t𝑥.\n",
    "\n",
    "First notice that the 𝑩\\_t𝑥 quantity is not dependent on previous hidden states; it can be calculated as a simple matrix element-wise multiplication for all t before running the prefix sum algorithm.\n",
    "\n",
    "The next thing we need to do is shift our “prefix sum” approach to a more general “prefix scan” approach. The prefix scan is a more general version of the prefix sum, where an accumulator function abstraction are introduced. For prefix sums, this is the addition operator. The semantics here are equivalent to that of the itertools.accumulate function in Python.\n",
    "\n",
    "So our goal now is to rewrite ℎ𝑡 \\= 𝑨\\_tℎ𝑡−1 \\+ 𝑩\\_t𝑥 in a form that can be computed using the up sweep/down sweep algorithm described above. This means that we need to be able to write h\\_t as the partial accumulations of h\\[0:i\\] and h\\[i:t\\], essentially requiring us to define the accumulation functions.\n",
    "\n",
    "The final accumulation of h\\_t is actually the sum(h\\[i:t\\]) \\+ prod(A\\[i:t\\]) \\* sum(h\\[0:i\\]). To see why this is the case, we can try the simple case of computing h\\_1 with i \\= 1:\n",
    "\n",
    "h\\_1 \\= 𝑨ℎ\\_0 \\+ 𝑩𝑥\\_1 \\= 𝑨\\_1Bx\\_0 \\+ 𝑩𝑥\\_1 \\= prod() 𝑩𝑥\\_1\n",
    "\n",
    "with h\\_0 being just Bx\\_0\n",
    "\n",
    "Based on this, we argue that calculating the prefix scan of h\\_1 is actually two parallel prefix scans: one for keeping track of the partial products in the A array, and the other for actually keeping the partial accumulations of the hidden states.\n",
    "\n",
    "Therefore, the accumulation function for the A prefix scan is just the multiplication operator. And for the hidden states, it’s actually just the sum(h\\[i:t\\]) \\+ prod(A\\[i:t\\]) \\* sum(h\\[0:i\\]).\n",
    "\n",
    "One last implementation detail here is that we require an inclusive prefix scan rather than an exclusive prefix scan . We notice that the\n",
    "\n",
    "In order to test the running time of this algorithm versus the sequential prefix scan implementation, we ran a simple experiment where we initialized random A and X matrices and ran each scan implementation 1,000 times with different sequence lengths, keeping the rest of the dimensions constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "746gTvndb2wP",
    "outputId": "d36ccde6-489f-49e3-b421-7ddc1bb3772e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA pscan impelmentation (s): 0.6208266129999629\n",
      "numpy sequential scan benchmark (s): 1.9076007060000393\n",
      "testing correctness\n",
      "correct\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import needle as ndl\n",
    "\n",
    "B, D, N = 10, 28, 10\n",
    "device = ndl.cuda()\n",
    "\n",
    "L = 64 # sequence length\n",
    "\n",
    "A_n = np.random.rand(B, D, L, N)\n",
    "X_n = np.random.rand(B, D, L, N)\n",
    "\n",
    "A = ndl.Tensor(A_n, device=device)\n",
    "X = ndl.Tensor(X_n, device=device)\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(1_000):\n",
    "  y = A.cached_data.pscan(X.cached_data).numpy()\n",
    "print(f\"CUDA pscan impelmentation (s):\", perf_counter() - start)\n",
    "\n",
    "start = perf_counter()\n",
    "for _ in range(1_000):\n",
    "  result = np.zeros((B, D, L, N))\n",
    "  result[:, :, 0, :] = X_n[:, :, 0, :]\n",
    "  for i in range(1, L):\n",
    "    result[:, :, i, :] = (result[:, :, i - 1, :] * A_n[:, :, i, :]) + X_n[\n",
    "      :, :, i, :\n",
    "    ]\n",
    "\n",
    "print(f\"numpy sequential scan benchmark (s):\", perf_counter() - start)\n",
    "\n",
    "print(\"testing correctness\")\n",
    "np.testing.assert_allclose(y, result, atol=1e-5, rtol=1e-5)\n",
    "print(\"correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BirLATPXb7Fy"
   },
   "source": [
    "Even for a sequence length of 64, we saw that the parallel CUDA version was about 2-3 times faster than the sequential version. Of course, we expect the gap widens as the sequence length increases given the logarithmic time complexity the parallel algorithm provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX80nSmzYc7L"
   },
   "source": [
    "Finally, we acknowledge that the prefix scan algorithm, as we implemented it in CUDA, has a few opportunities for improvement that we can work on in the future:\n",
    "\n",
    "* currently only supports small sequence lengths (L \\<= 128\\) given that we try to compute the prefix sum for an entire channel within a single thread block  \n",
    "* currently only supports sequence lengths that are powers of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "almXUZwCYc7L"
   },
   "source": [
    "# **Using Mamba in Needle**\n",
    "\n",
    "add code snippet to instantiate mamba model, and explain hyperparameters\n",
    "\n",
    "Add training code\n",
    "\n",
    "Add generating code\n",
    "\n",
    "Add plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MXonLUTbfVi0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import MambaLM, MambaLMConfig\n",
    "import needle as ndl\n",
    "from simple_ml import evaluate_ptb, train_ptb, generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nGjoXL__fbtc"
   },
   "outputs": [],
   "source": [
    "device = ndl.cuda()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\", max_lines=100)\n",
    "train_data = ndl.data.batchify(\n",
    "  corpus.train, batch_size=8, device=device, dtype=\"float32\"\n",
    ")\n",
    "\n",
    "config = MambaLMConfig(d_model=16, n_layers=4, vocab_size=len(corpus.dictionary))\n",
    "\n",
    "def train_and_evaluate(config):\n",
    "  model = MambaLM(config, device=device)\n",
    "  train_errors, train_losses = train_ptb(\n",
    "    model,\n",
    "    train_data,\n",
    "    seq_len=16,\n",
    "    n_epochs=10,\n",
    "    device=device,\n",
    "    lr=0.003,\n",
    "    optimizer=ndl.optim.Adam,\n",
    "  )\n",
    "  evaluate_ptb(model, train_data, seq_len=16, device=device)\n",
    "\n",
    "  generated_text = generate_text(model, train_data, corpus.dictionary, seq_len=16, device=device, dtype=\"float32\")\n",
    "  return train_errors, train_losses, generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "w-k5zKKoeY3_",
    "outputId": "970e2144-f9fe-413d-e4d9-daee01555c1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_batch_loss:  13.637539446353912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:02<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_batch_loss:  12.099065005779266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:03<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_batch_loss:  11.087206721305847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:04<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_batch_loss:  10.379030287265778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [00:16<00:55,  4.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-23c15b55b5c0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mseq_train_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_train_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-05f53e34b9a9>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMambaLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   train_errors, train_losses = train_ptb(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./apps/simple_ml.py\u001b[0m in \u001b[0;36mtrain_ptb\u001b[0;34m(model, data, seq_len, n_epochs, optimizer, lr, weight_decay, loss_fn, clip, device, dtype)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mtrain_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         train_err, train_loss = epoch_general_ptb(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./apps/simple_ml.py\u001b[0m in \u001b[0;36mepoch_general_ptb\u001b[0;34m(data, model, seq_len, loss_fn, opt, clip, device, dtype)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/autograd.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         )\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mcompute_gradient_of_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# def __getitem__(self, index):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/autograd.py\u001b[0m in \u001b[0;36mcompute_gradient_of_variables\u001b[0;34m(output_tensor, out_grad)\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_to_output_grads_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mnode_to_output_grads_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/autograd.py\u001b[0m in \u001b[0;36mgradient_as_tuple\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\"Convenience method to always return a tuple from gradient call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/ops/ops_mathematic.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, out_grad, node)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/ops/ops_mathematic.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(args, axis)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/autograd.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_from_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/autograd.py\u001b[0m in \u001b[0;36mmake_from_op\u001b[0;34m(op, inputs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/autograd.py\u001b[0m in \u001b[0;36mrealize_cached_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# note: data implicitly calls realized cached data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         self.cached_data = self.op.compute(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         )\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/ops/ops_mathematic.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/backend_ndarray/ndarray.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, idxs, other)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             self.device.ewise_setitem(\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/backend_ndarray/ndarray.py\u001b[0m in \u001b[0;36mcompact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;34m\"\"\"Convert a matrix to be compact\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/backend_ndarray/ndarray.py\u001b[0m in \u001b[0;36mis_compact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         of the shape dimensions\"\"\"\n\u001b[1;32m    206\u001b[0m         return (\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact_strides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         )\n",
      "\u001b[0;32m/content/mamba_needle/./python/needle/backend_ndarray/ndarray.py\u001b[0m in \u001b[0;36mcompact_strides\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstride\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config.pscan = False\n",
    "seq_train_errors, seq_train_losses, seq_generated_text = train_and_evaluate(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XrQ4jRqe79E"
   },
   "outputs": [],
   "source": [
    "config.pscan = True\n",
    "pscan_train_errors, pscan_train_losses, pscan_genereated_text = train_and_evaluate(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GpGDBzPgc1d"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_train_losses, label=\"Sequential Loss\", color=\"blue\")\n",
    "plt.plot(pscan_train_losses, label=\"Parallel Scan Loss\", color=\"orange\")\n",
    "plt.title(\"Training Loss Comparison\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kdhoHuiiEW7"
   },
   "outputs": [],
   "source": [
    "print(\"Sequential Generated Text:\")\n",
    "print(seq_generated_text)\n",
    "print(\"\\nParallel Scan Generated Text:\")\n",
    "print(pscan_genereated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-HdMIvaYc7L"
   },
   "source": [
    "# **Future Work**\n",
    "\n",
    "* Making the code hardware aware : A disadvantage of recent GPUs is their limited transfer (IO) speed between their small but highly efficient SRAM and their large but slightly less efficient DRAM. So, instead of preparing the scan input (𝑨, 𝑩) of size (B, L, D, N) in GPU HBM (high-bandwidth memory), load the SSM parameters (Δ, 𝑨, 𝑩, 𝑪) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1paIKyGdYc7L"
   },
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JE1Y3e7Yc7L"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
