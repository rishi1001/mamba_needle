{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mamba in Needle** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Mamba Architecture](images/mamba.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A State-Space Model Architecture**\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In the homeworks for this course, we implemented three sequence models in the Needle library: recurrent neural networks (RNNs), long short-term memory (LSTM) networks and transformers. In this project, we add a fourth model to this list: Mamba.\n",
    "\n",
    "Mamba is a state-space model architecture designed to combine the high accuracy of transformers with the efficiency of linear RNNs. This report explains the key principles behind Mamba, its efficiency, scalability, and how it achieves constant time inference.\n",
    "\n",
    "### **Efficiency**\n",
    "\n",
    "* **Linear Time Training**: Unlike transformers, which scale quadratically with sequence length, Mamba optimizes training to be linear.  \n",
    "* **Constant Time Inference**: Enables real-time applications by reducing computational overhead.\n",
    "\n",
    "### **Scalability**\n",
    "\n",
    "* **Long-Range Tasks**: Handles long sequences efficiently.  \n",
    "* **Fixed Computation per Timestep**: Ensures scalability while avoiding quadratic complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-space model (SSM) forms the backbone of Mamba's architecture:\n",
    "\n",
    "1. **State Equation**: \\[ h'(t) \\= A h(t) \\+ B x(t) \\]  \n",
    "2. **Output Equation**: \\[ y(t) \\= C h(t) \\+ D x(t) \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![SSM Equations](images/ssm_equations.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state equation describes how the state changes as a function of time and as a function of its input x.\n",
    "\n",
    "* **h‚Äô(t)**: the state update  \n",
    "* **Ah(t)**: how the current state changes over time  \n",
    "* **Bx(t)**: how the input influences the state\n",
    "\n",
    "The output equation describes how the output changes as a function of the state and as a function of its input x.\n",
    "\n",
    "* **y(t)**: the output  \n",
    "* **Ch(t)**: how the current state affects the output  \n",
    "* **Dx(t)**: how the input directly influences the output\n",
    "\n",
    "We transform the continuous h(t) from above to a discrete form. A continuous h(t) is difficult to work with and also does not represent the input since inputs are usually discrete (i.e. sequence of text). This process can be compared to transforming the area under a curve to a Riemann sum. The discretized matrices A and B are calculated using the formula shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now, we have the discretized version : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SSMs can be calculated in the form of kernels. We can precaulcate the kernels and apply them parallely over the input, similar to CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mamba Block**\n",
    "\n",
    "The Mamba model is composed of a sequence of Mamba blocks. The architecture of the Mamba block is shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Mamba Block](images/mamba_block.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the following blocks:\n",
    "\n",
    "* **RMSNorm**: Applied over the last (not batch size or sequence length) dimension. Divides each input by the L2 norm of the last dimension. A small value (epsilon) can be added to the denominator to ensure there will be no divide by 0 errors.  \n",
    "* **Conv1D**: While Conv2D was applied over the last 2 dimensions, Conv1D is applied over the last dimension. It takes input of shape (batch\\_size, in\\_channels, sequence\\_length) and slides a kernel of size (kernel\\_size, in\\_channels, out\\_channels) over the length of the sequence. We added a groups argument, which determines how many groups to divide the inputs and outputs into. Each group is convolved separately, so the inputs only affect outputs in the same group.  \n",
    "* **SiLU**: Input times logistic sigmoid.   \n",
    "* **Selective SSM** (see next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selective SSM**\n",
    "\n",
    "SSMs are poor at selectively remembering specific inputs. For instance, a SSM would fail at copying specific parts of an input and outputting them in order. This is because the matrices A, B, and C are the same for each token the SSM generates, resulting in the inability to treat tokens differently. \n",
    "\n",
    "A selective SSM solves this problem by making dt, B, and C dependent on the input. Recall that Bx(t) represents how the input influences the state and Ch(t) represents how the current state affects the output. The architecture for the Selective SSM is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Selective SSM Block](images/selective_ssm.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sequential Selective Scan**\n",
    "\n",
    "The selective scan implements the updates to the state equation and the output equation (see State Space Models section). The state equation is dependent on the previous state, which allows us to formulate the equation updates as a RNN as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, since these matrices are now dynamic, they cannot be calculated using the convolution representation since it assumes a fixed kernel(like in SSMs). We can only use the recurrent representation and lose the parallelization the convolution provides. So, we use a prefix parallel scan algorithm during training as described ahead, and recurrent representation while inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parallel prefix scans**\n",
    "\n",
    "So far, we have seen an implementation of a Mamba that performs the forward pass sequentially; in linear time with respect to the length of the input sequence. According to the equations provided above, this is the natural implementation. And even this implementation provides time complexity improvements over the quadratic time transformer model.\n",
    "\n",
    "However, the authors of the mamba paper argued that the computations can also equivalently be written as prefix sums. And given previous research on parallel prefix sum algorithms, they showed that they could further reduce the time complexity.\n",
    "\n",
    "The parallel prefix sum algorithm we used was initially described by CMU Professor Guy Blelloch and further documented on NVIDIA‚Äôs website\\[1\\]. It‚Äôs divided up into two phases: the ‚Äúup sweep‚Äù phase and the ‚Äúdown sweep‚Äù phase.\n",
    "\n",
    "Each phase has on the order of log\\_2(L) steps, where L is the length of the input sequence. And the actual number of computations done across all steps ‚Äî referred to on the website as ‚Äúwork efficiency‚Äù ‚Äî is also linear, meaning that asymptotically no extra computations are done in this algorithm versus the sequential implementation.\n",
    "\n",
    "With all this being said, we needed to augment this base integer prefix sum algorithm in two ways. First: we needed to add support for multiple dimensions given that our input isn‚Äôt just a 1D array.\n",
    "\n",
    "The solution for the first problem is simple: we just launch a separate thread block for every dimension that we compute like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second problem is that we‚Äôre not just computing a basic prefix sum; each step includes a multiplication and an addition. Let‚Äôs take a second look at the to make this more concrete:\n",
    "\n",
    "‚Ñéùë° \\= ùë®\\_t‚Ñéùë°‚àí1 \\+ ùë©\\_tùë•.\n",
    "\n",
    "First notice that the ùë©\\_tùë• quantity is not dependent on previous hidden states; it can be calculated as a simple matrix element-wise multiplication for all t before running the prefix sum algorithm.\n",
    "\n",
    "The next thing we need to do is shift our ‚Äúprefix sum‚Äù approach to a more general ‚Äúprefix scan‚Äù approach. The prefix scan is a more general version of the prefix sum, where an accumulator function abstraction are introduced. For prefix sums, this is the addition operator. The semantics here are equivalent to that of the itertools.accumulate function in Python.\n",
    "\n",
    "So our goal now is to rewrite ‚Ñéùë° \\= ùë®\\_t‚Ñéùë°‚àí1 \\+ ùë©\\_tùë• in a form that can be computed using the up sweep/down sweep algorithm described above. This means that we need to be able to write h\\_t as the partial accumulations of h\\[0:i\\] and h\\[i:t\\], essentially requiring us to define the accumulation functions.\n",
    "\n",
    "The final accumulation of h\\_t is actually the sum(h\\[i:t\\]) \\+ prod(A\\[i:t\\]) \\* sum(h\\[0:i\\]). To see why this is the case, we can try the simple case of computing h\\_1 with i \\= 1:\n",
    "\n",
    "h\\_1 \\= ùë®‚Ñé\\_0 \\+ ùë©ùë•\\_1 \\= ùë®\\_1Bx\\_0 \\+ ùë©ùë•\\_1 \\= prod() ùë©ùë•\\_1\n",
    "\n",
    "with h\\_0 being just Bx\\_0\n",
    "\n",
    "Based on this, we argue that calculating the prefix scan of h\\_1 is actually two parallel prefix scans: one for keeping track of the partial products in the A array, and the other for actually keeping the partial accumulations of the hidden states.\n",
    "\n",
    "Therefore, the accumulation function for the A prefix scan is just the multiplication operator. And for the hidden states, it‚Äôs actually just the sum(h\\[i:t\\]) \\+ prod(A\\[i:t\\]) \\* sum(h\\[0:i\\]).\n",
    "\n",
    "One last implementation detail here is that we require an inclusive prefix scan rather than an exclusive prefix scan . We notice that the \n",
    "\n",
    "In order to test the running time of this algorithm versus the sequential prefix scan implementation, we ran a simple experiment where we initialized random A and X matrices and ran each scan implementation 1,000 times with different sequence lengths, keeping the rest of the dimensions constant:\n",
    "\n",
    "\\<CODE\\>\n",
    "\n",
    "\\<GRAPH\\>\n",
    "\n",
    "Even for a sequence length of 64, we saw that the parallel CUDA version was about 2-3 times faster than the sequential version. Of course, we expect the gap widens as the sequence length increases given the logarithmic time complexity the parallel algorithm provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we acknowledge that the prefix scan algorithm, as we implemented it in CUDA, has a few opportunities for improvement that we can work on in the future:\n",
    "\n",
    "* currently only supports small sequence lengths (L \\<= 128\\) given that we try to compute the prefix sum for an entire channel within a single thread block  \n",
    "* currently only supports sequence lengths that are powers of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using Mamba in Needle**\n",
    "\n",
    "add code snippet to instantiate mamba model, and explain hyperparameters\n",
    "\n",
    "Add training code\n",
    "\n",
    "Add generating code\n",
    "\n",
    "Add plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Future Work**\n",
    "\n",
    "* Making the code hardware aware : A disadvantage of recent GPUs is their limited transfer (IO) speed between their small but highly efficient SRAM and their large but slightly less efficient DRAM. So, instead of preparing the scan input (ùë®, ùë©) of size (B, L, D, N) in GPU HBM (high-bandwidth memory), load the SSM parameters (Œî, ùë®, ùë©, ùë™) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
